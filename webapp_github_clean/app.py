import streamlit as st
import sys
import os
import cv2
import numpy as np
import torch
import torchvision.transforms as T
from PIL import Image
from torch.utils.data import DataLoader, Dataset
import traceback
import datetime
import json
import pandas as pd
from pathlib import Path
import gdown  # Th∆∞ vi·ªán ƒë·ªÉ t·∫£i file t·ª´ Drive

# --- 1. SETUP & IMPORT CONFIG ---
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)

try:
    import config
except ImportError:
    st.error("‚ùå Kh√¥ng t√¨m th·∫•y file config.py. Vui l√≤ng ki·ªÉm tra l·∫°i th∆∞ m·ª•c.")
    st.stop()

sys.path.append(str(config.SRC_DIR))

# --- 2. C·∫§U H√åNH TRANG ---
st.set_page_config(
    page_title=config.APP_TITLE,
    page_icon=config.APP_ICON,
    layout="wide",
    initial_sidebar_state="expanded"
)

st.markdown("""
    <style>
        .stApp {background-color: #f8f9fa;}
        div[data-testid="stMetricValue"] {font-size: 1.2rem; font-weight: bold;}
        .block-container {padding-top: 2rem;}
        div[data-testid="stDataFrame"] {font-size: 0.85rem;}
    </style>
""", unsafe_allow_html=True)

# ============================================================
# üì• T·ª∞ ƒê·ªòNG T·∫¢I MODEL T·ª™ GOOGLE DRIVE
# ============================================================
# ‚ö†Ô∏è THAY ID C·ª¶A B·∫†N V√ÄO D∆Ø·ªöI ƒê√ÇY (ID l·∫•y t·ª´ link share file .pth)
MODEL_DRIVE_ID = "1Ruvjg57t-JLoP1QcWK_I8UzcFuUFjCnN" 

@st.cache_resource
def download_model_from_drive():
    """T·∫£i model t·ª´ Google Drive n·∫øu ch∆∞a t·ªìn t·∫°i"""
    if not config.MODEL_PATH.exists():
        # T·∫°o th∆∞ m·ª•c models n·∫øu ch∆∞a c√≥
        config.MODELS_DIR.mkdir(parents=True, exist_ok=True)
        
        url = f'https://drive.google.com/uc?id={MODEL_DRIVE_ID}'
        output = str(config.MODEL_PATH)
        
        st.toast("‚è≥ ƒêang t·∫£i Model t·ª´ Cloud (L·∫ßn ƒë·∫ßu ch·∫°y m·∫•t ~1 ph√∫t)...", icon="cloud")
        try:
            gdown.download(url, output, quiet=False)
            st.success("‚úÖ T·∫£i Model th√†nh c√¥ng!")
        except Exception as e:
            st.error(f"‚ùå L·ªói t·∫£i model: {e}")
            st.stop()

# ============================================================
# 3. CLASS & CORE FUNCTIONS
# ============================================================

class WSIPatchDataset(Dataset):
    def __init__(self, image, coords, patch_size=50, transform=None):
        self.image = image
        self.coords = coords
        self.patch_size = patch_size
        self.transform = transform

    def __len__(self):
        return len(self.coords)

    def __getitem__(self, idx):
        y, x = self.coords[idx]
        patch = self.image[y : y + self.patch_size, x : x + self.patch_size]
        if patch.shape[0] != self.patch_size or patch.shape[1] != self.patch_size:
            patch = cv2.resize(patch, (self.patch_size, self.patch_size))
        if self.transform:
            patch = self.transform(patch)
        return patch

@st.cache_resource
def load_model(device_name):
    """Load model"""
    # ƒê·∫£m b·∫£o model ƒë√£ ƒë∆∞·ª£c t·∫£i v·ªÅ tr∆∞·ªõc khi load
    download_model_from_drive()
    
    device = torch.device(device_name)
    try:
        from src.model_hybrid1 import CNNDeiTSmall
        model = CNNDeiTSmall(**config.MODEL_PARAMS)
        
        if not config.MODEL_PATH.exists():
            st.error(f"‚ùå File model kh√¥ng t·ªìn t·∫°i: {config.MODEL_PATH}")
            return None
            
        checkpoint = torch.load(config.MODEL_PATH, map_location=device)
        state_dict = checkpoint['model_state'] if 'model_state' in checkpoint else checkpoint
        model.load_state_dict(state_dict, strict=False)
        model.to(device)
        model.eval()
        return model
    except Exception as e:
        st.error(f"L·ªói kh·ªüi t·∫°o Model: {e}")
        return None

def generate_tissue_mask(img_rgb):
    img_hsv = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2HSV)
    lower_white = np.array([0, 0, 230])
    upper_white = np.array([180, 25, 255]) 
    mask_white = cv2.inRange(img_hsv, lower_white, upper_white)
    tissue_mask = cv2.bitwise_not(mask_white)
    kernel = np.ones((5,5), np.uint8)
    tissue_mask = cv2.dilate(tissue_mask, kernel, iterations=1)
    return tissue_mask

def run_inference(model, image_array, device, threshold, batch_size, max_patches, progress_bar):
    h, w = image_array.shape[:2]
    patch_size = config.PATCH_SIZE
    stride = config.STRIDE
    
    tissue_mask = generate_tissue_mask(image_array)
    coords = []
    for y in range(0, h - patch_size + 1, stride):
        for x in range(0, w - patch_size + 1, stride):
            mask_roi = tissue_mask[y : y + patch_size, x : x + patch_size]
            if cv2.countNonZero(mask_roi) / (patch_size**2) > 0.05:
                coords.append((y, x))
    
    if not coords:
        return None, None, {"cancer_percentage": 0.0}

    total_found = len(coords)
    if max_patches > 0 and total_found > max_patches:
        coords = coords[:max_patches]
        st.toast(f"‚ö° Gi·ªõi h·∫°n x·ª≠ l√Ω: {max_patches}/{total_found} patches", icon="üöÄ")

    transform = T.Compose([
        T.ToPILImage(),
        T.Resize(config.MODEL_PARAMS['img_size']),
        T.ToTensor(),
        T.Normalize(mean=config.NORMALIZE_MEAN, std=config.NORMALIZE_STD)
    ])
    
    dataset = WSIPatchDataset(image_array, coords, patch_size, transform)
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=config.NUM_WORKERS)
    
    predictions = []
    confidences = []
    
    with torch.no_grad():
        for i, batch in enumerate(loader):
            batch = batch.to(device)
            outputs = model(batch)
            probs = torch.softmax(outputs, dim=1)[:, 1]
            predictions.extend((probs >= threshold).int().cpu().numpy())
            confidences.extend(probs.cpu().numpy())
            if progress_bar:
                prog_val = (i + 1) / len(loader)
                progress_bar.progress(prog_val, text=f"ƒêang x·ª≠ l√Ω batch {i+1}/{len(loader)}...")

    heatmap = np.zeros((h, w), dtype=np.float32)
    overlay = image_array.copy()
    cancer_count = 0
    
    for (y, x), pred, conf in zip(coords, predictions, confidences):
        heatmap[y : y + patch_size, x : x + patch_size] = conf
        if pred == 1:
            cancer_count += 1
            cv2.rectangle(overlay, (x, y), (x + patch_size, y + patch_size), (255, 0, 0), 2)
            
    stats = {
        "total_patches": len(coords),
        "original_patches": total_found,
        "cancer_patches": cancer_count,
        "cancer_percentage": round((cancer_count / len(coords)) * 100, 2),
        "max_confidence": round(float(np.max(confidences)), 4) if confidences else 0
    }
    
    return overlay, heatmap, stats

# ============================================================
# 4. GIAO DI·ªÜN NG∆Ø·ªúI D√ôNG (MAIN)
# ============================================================
def main():
    # G·ªçi h√†m t·∫£i model ngay khi app kh·ªüi ƒë·ªông
    download_model_from_drive()

    if 'analysis_result' not in st.session_state:
        st.session_state.analysis_result = None
    if 'history' not in st.session_state:
        st.session_state.history = []

    # === SIDEBAR ===
    with st.sidebar:
        if config.LOGO_PATH and config.LOGO_PATH.exists():
            st.image(str(config.LOGO_PATH), width=120)
        
        st.header("‚öôÔ∏è C·∫•u h√¨nh h·ªá th·ªëng")
        # √âp hi·ªÉn th·ªã CPU n·∫øu ch·∫°y tr√™n Cloud (th∆∞·ªùng Cloud free kh√¥ng c√≥ GPU)
        device_display = "CPU (Cloud)" if not torch.cuda.is_available() else "GPU (CUDA)"
        st.info(f"Thi·∫øt b·ªã: **{device_display}**")

        with st.expander("üõ†Ô∏è Tham s·ªë M√¥ h√¨nh", expanded=False):
            st.markdown(f"- Model: Hybrid CNN-DeiT\n- Patch Size: {config.PATCH_SIZE}px")
            if hasattr(config, 'MODEL_VIZ_PATH') and config.MODEL_VIZ_PATH.exists():
                st.image(str(config.MODEL_VIZ_PATH), caption="Ki·∫øn tr√∫c ƒë·ªÅ xu·∫•t", use_column_width=True)
            
            ui_max_patches = st.slider("‚ö° Gi·ªõi h·∫°n Patch (Demo)", 0, 5000, 0, 100)

        ui_threshold = st.slider("Ng∆∞·ª°ng (Threshold)", 0.0, 1.0, config.CONFIDENCE_THRESHOLD, 0.05)
        
        default_bs_idx = 0 # Cloud n√™n ƒë·ªÉ batch nh·ªè (16) ƒë·ªÉ tr√°nh tr√†n RAM
        ui_batch_size = st.selectbox("Batch Size", [16, 32, 64, 128], index=default_bs_idx)

        if st.session_state.history:
            st.markdown("---")
            st.subheader("üïí L·ªãch s·ª≠ phi√™n n√†y")
            st.dataframe(pd.DataFrame(st.session_state.history), use_container_width=True, hide_index=True)

        st.caption("¬© 2026 V≈© H·ªØu Ho√†ng")

    # === MAIN CONTENT ===
    st.title(config.APP_TITLE)
    st.write("---")

    col1, col2 = st.columns([1, 1.5])

    with col1:
        st.subheader("1. T·∫£i ·∫£nh ƒë·∫ßu v√†o")
        input_source = st.radio("Ngu·ªìn ·∫£nh:", ["T·∫£i ·∫£nh l√™n", "D√πng ·∫£nh m·∫´u (Demo)"], horizontal=True)
        uploaded_file = None
        current_img_name = ""

        if input_source == "T·∫£i ·∫£nh l√™n":
            uploaded_file = st.file_uploader("Ch·ªçn ·∫£nh H&E (JPG, PNG)", type=["jpg", "png", "jpeg"])
            if uploaded_file:
                current_img_name = uploaded_file.name
                image_pil = Image.open(uploaded_file).convert('RGB')
        else:
            if hasattr(config, 'SAMPLE_IMAGES'):
                sample_choice = st.selectbox("Ch·ªçn ca b·ªánh m·∫´u:", list(config.SAMPLE_IMAGES.keys()))
                sample_path = config.SAMPLE_IMAGES[sample_choice]
                if sample_path.exists():
                    image_pil = Image.open(sample_path).convert('RGB')
                    current_img_name = sample_path.name
                    class MockFile: name = current_img_name
                    uploaded_file = MockFile()

        if 'image_pil' in locals() and image_pil:
            image_array = np.array(image_pil)
            st.image(image_pil, caption=f"·∫¢nh ƒë·∫ßu v√†o: {current_img_name}", use_column_width=True)
            analyze_trigger = st.button("üöÄ PH√ÇN T√çCH NGAY", type="primary", use_container_width=True)
        else:
            analyze_trigger = False

    with col2:
        st.subheader("2. K·∫øt qu·∫£ Ch·∫©n ƒëo√°n")
        if uploaded_file and analyze_trigger:
            progress_bar = st.progress(0, text="Kh·ªüi t·∫°o m√¥ h√¨nh...")
            try:
                # Cloud th∆∞·ªùng kh√¥ng c√≥ GPU, √©p d√πng CPU n·∫øu c·∫ßn
                run_device = "cuda" if torch.cuda.is_available() else "cpu"
                model = load_model(run_device)
                
                if model:
                    overlay, heatmap, stats = run_inference(
                        model, image_array, run_device, 
                        ui_threshold, ui_batch_size, ui_max_patches, progress_bar
                    )
                    progress_bar.empty()
                    
                    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
                    st.session_state.analysis_result = {
                        'overlay': overlay, 'heatmap': heatmap, 'stats': stats,
                        'filename': current_img_name, 'timestamp': timestamp
                    }
                    st.session_state.history.insert(0, {"Th·ªùi gian": datetime.datetime.now().strftime("%H:%M"), "·∫¢nh": current_img_name, "T·ª∑ l·ªá": f"{stats['cancer_percentage']}%"})

            except Exception as e:
                st.error("L·ªói h·ªá th·ªëng."); st.code(traceback.format_exc())

        result = st.session_state.analysis_result
        if result and uploaded_file and result['filename'] == current_img_name:
            # (Ph·∫ßn hi·ªÉn th·ªã k·∫øt qu·∫£ gi·ªØ nguy√™n nh∆∞ c≈©)
            overlay, heatmap, stats, timestamp = result['overlay'], result['heatmap'], result['stats'], result['timestamp']
            
            if overlay is None:
                st.warning("Kh√¥ng t√¨m th·∫•y m√¥ t·∫ø b√†o.")
            else:
                st.info(f"K·∫øt qu·∫£: **{result['filename']}**")
                tab1, tab2 = st.tabs(["V√πng t·ªïn th∆∞∆°ng", "B·∫£n ƒë·ªì nhi·ªát"])
                heatmap_vis = (np.clip(heatmap, 0, 1) * 255).astype(np.uint8)
                heatmap_color = cv2.cvtColor(cv2.applyColorMap(heatmap_vis, cv2.COLORMAP_JET), cv2.COLOR_BGR2RGB)
                blend = cv2.addWeighted(image_array, 0.6, heatmap_color, 0.4, 0)

                with tab1: st.image(overlay, caption="Ph√°t hi·ªán IDC", use_column_width=True)
                with tab2: st.image(blend, caption="Heatmap", use_column_width=True)

                st.divider()
                c1, c2, c3, c4 = st.columns(4)
                c1.metric("T·ªïng Patch", stats['total_patches'])
                c2.metric("IDC Patch", stats['cancer_patches'])
                c3.metric("T·ª∑ l·ªá b·ªánh", f"{stats['cancer_percentage']}%")
                c4.metric("Max Conf", stats['max_confidence'])

                if stats['cancer_percentage'] >= config.DANGER_THRESHOLD_PERCENT:
                    st.error(f"üö® NGUY C∆† CAO")
                else:
                    st.success("‚úÖ AN TO√ÄN")

if __name__ == "__main__":
    main()